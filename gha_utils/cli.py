# Copyright Kevin Deldycke <kevin@deldycke.com> and contributors.
#
# This program is Free Software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.

from __future__ import annotations

import logging
import os
import re
import sys
from collections import Counter
from datetime import datetime
from pathlib import Path

from boltons.iterutils import unique
from click_extra import (
    Choice,
    Context,
    EnumChoice,
    FloatRange,
    IntRange,
    argument,
    echo,
    file_path,
    group,
    option,
    pass_context,
)
from click_extra.envvar import merge_envvar_ids
from extra_platforms import ALL_IDS, is_github_ci

from . import __version__
from .changelog import Changelog
from .bundled_config import (
    EXPORTABLE_FILES,
    INIT_CONFIGS,
    export_content,
    get_default_output_path,
    init_config,
)
from .mailmap import Mailmap
from .metadata import NUITKA_BUILD_TARGETS, Dialect, Metadata, is_version_bump_allowed
from .release_prep import ReleasePrep
from .sponsor import (
    add_sponsor_label,
    get_default_author,
    get_default_number,
    get_default_owner,
    get_default_repo,
    is_pull_request,
    is_sponsor,
)
from .test_plan import DEFAULT_TEST_PLAN, SkippedTest, parse_test_plan

TYPE_CHECKING = False
if TYPE_CHECKING:
    from typing import IO


def is_stdout(filepath: Path) -> bool:
    """Check if a file path is set to stdout.

    Prevents the creation of a ``-`` file in the current directory.
    """
    return str(filepath) == "-"


def prep_path(filepath: Path) -> IO | None:
    """Prepare the output file parameter for Click's echo function."""
    if is_stdout(filepath):
        return None
    return filepath.open("w", encoding="UTF-8")


def generate_header(ctx: Context) -> str:
    """Generate metadata to be left as comments to the top of a file generated by
    this CLI.
    """
    header = (
        f"# Generated by {ctx.command_path} v{__version__}"
        " - https://github.com/kdeldycke/workflows\n"
        f"# Timestamp: {datetime.now().isoformat()}\n"
    )
    logging.debug(f"Generated header:\n{header}")
    return header


def remove_header(content: str) -> str:
    """Return the same content provided, but without the blank lines and header metadata generated by the function above."""
    logging.debug(f"Removing header from:\n{content}")
    lines = []
    still_in_header = True
    for line in content.splitlines():
        if still_in_header:
            # We are still in the header as long as we have blank lines or we have
            # comment lines matching the format produced by the method above.
            if not line.strip() or line.startswith((
                "# Generated by ",
                "# Timestamp: ",
            )):
                continue
            else:
                still_in_header = False
        # We are past the header, so keep all the lines: we have nothing left to remove.
        lines.append(line)

    headerless_content = "\n".join(lines)
    logging.debug(f"Result of header removal:\n{headerless_content}")
    return headerless_content


@group
def gha_utils():
    pass


@gha_utils.command(short_help="Output project metadata")
@option(
    "-u",
    "--unstable-targets",
    help="Build targets for which Nuitka is allowed to fail without comprimising the "
    " release workflow. This option accepts a mangled string with multiple targets "
    "separated by arbitrary separators. Recognized targets are: "
    f"{', '.join(NUITKA_BUILD_TARGETS)}.",
)
@option(
    "--format",
    type=EnumChoice(Dialect),
    default=Dialect.github,
    help="Rendering format of the metadata.",
)
@option(
    "--overwrite/--no-overwrite",
    "--force/--no-force",
    "--replace/--no-replace",
    default=True,
    help="Overwrite output file if it already exists.",
)
@option(
    "-o",
    "--output",
    type=file_path(writable=True, resolve_path=True, allow_dash=True),
    default="-",
    help="Output file path. Defaults to stdout.",
)
@pass_context
def metadata(ctx, unstable_targets, format, overwrite, output):
    """Dump project metadata to a file.

    By default the metadata produced are displayed directly to the console output.
    To have the results written in a file on disk, specify the output file like so:
    `gha-utils metadata --output dump.txt`.

    For GitHub you want to output to the standard environment file pointed to by the
    `$GITHUB_OUTPUT` variable. I.e.:

        $ gha-utils metadata --output "$GITHUB_OUTPUT"
    """
    if is_stdout(output):
        if overwrite:
            logging.warning("Ignore the --overwrite/--force/--replace option.")
        logging.info(f"Print metadata to {sys.stdout.name}")
    else:
        logging.info(f"Dump all metadata to {output}")

        if output.exists():
            msg = "Target file exist and will be overwritten."
            if overwrite:
                logging.warning(msg)
            else:
                logging.critical(msg)
                ctx.exit(2)

    # Extract targets from the raw string provided by the user.
    valid_targets = set()
    if unstable_targets:
        for target in re.split(r"[^a-z0-9\-]", unstable_targets.lower()):
            if target:
                if target not in NUITKA_BUILD_TARGETS:
                    logging.fatal(
                        f"Unrecognized {target!r} target. "
                        f"Must be one of {', '.join(NUITKA_BUILD_TARGETS)}."
                    )
                    sys.exit(1)
                valid_targets.add(target)
        logging.debug(
            f"Parsed {unstable_targets!r} string into {valid_targets} targets."
        )

    metadata = Metadata(valid_targets)

    # Output a warning in GitHub runners if metadata are not saved to $GITHUB_OUTPUT.
    if is_github_ci():
        env_file = os.getenv("GITHUB_OUTPUT")
        if env_file and Path(env_file) != output:
            logging.warning(
                "Output path is not the same as $GITHUB_OUTPUT environment variable,"
                " which is generally what we're looking to do in GitHub CI runners for"
                " other jobs to consume the produced metadata."
            )

    dialect = Dialect(format)
    content = metadata.dump(dialect=dialect)
    echo(content, file=prep_path(output))


@gha_utils.command(short_help="Maintain a Markdown-formatted changelog")
@option(
    "--source",
    type=file_path(exists=True, readable=True, resolve_path=True),
    default="changelog.md",
    help="Changelog source file in Markdown format.",
)
@argument(
    "changelog_path",
    type=file_path(writable=True, resolve_path=True, allow_dash=True),
    default="-",
)
@pass_context
def changelog(ctx, source, changelog_path):
    initial_content = None
    if source:
        logging.info(f"Read initial changelog from {source}")
        initial_content = source.read_text(encoding="UTF-8")

    changelog = Changelog(initial_content)
    content = changelog.update()
    if content == initial_content:
        logging.warning("Changelog already up to date. Do nothing.")
        ctx.exit()

    if is_stdout(changelog_path):
        logging.info(f"Print updated results to {sys.stdout.name}")
    else:
        logging.info(f"Save updated results to {changelog_path}")
    echo(content, file=prep_path(changelog_path))


@gha_utils.command(short_help="Prepare files for a release")
@option(
    "--changelog",
    "changelog_path",
    type=file_path(exists=True, readable=True, writable=True, resolve_path=True),
    default="changelog.md",
    help="Path to the changelog file.",
)
@option(
    "--citation",
    "citation_path",
    type=file_path(readable=True, writable=True, resolve_path=True),
    default="citation.cff",
    help="Path to the citation file.",
)
@option(
    "--workflow-dir",
    default=".github/workflows",
    help="Path to the GitHub workflows directory.",
)
@option(
    "--default-branch",
    default="main",
    help="Name of the default branch for workflow URL updates.",
)
@option(
    "--update-workflows/--no-update-workflows",
    default=False,
    help="Update workflow URLs to use versioned tag instead of default branch.",
)
@option(
    "--post-release",
    is_flag=True,
    default=False,
    help="Run post-release steps (retarget workflow URLs to default branch).",
)
@pass_context
def release_prep(
    ctx,
    changelog_path,
    citation_path,
    workflow_dir,
    default_branch,
    update_workflows,
    post_release,
):
    """Prepare files for a release or post-release version bump.

    This command consolidates all release preparation steps:

    \b
    - Set release date in changelog (replaces "(unreleased)" with today's date).
    - Set release date in citation.cff.
    - Update changelog comparison URL from "...main" to "...v{version}".
    - Remove the "[!IMPORTANT]" warning block from changelog.
    - Optionally update workflow URLs to use versioned tag.

    For post-release (after the release commit), use --post-release to retarget
    workflow URLs back to the default branch.

    Examples:

    \b
        # Prepare release (changelog + citation)
        gha-utils release-prep

    \b
        # Prepare release including workflow URL updates
        gha-utils release-prep --update-workflows

    \b
        # Post-release: retarget workflows to main branch
        gha-utils release-prep --post-release --update-workflows
    """
    workflow_dir_path = Path(workflow_dir).resolve() if workflow_dir else None
    prep = ReleasePrep(
        changelog_path=changelog_path,
        citation_path=citation_path if citation_path.exists() else None,
        workflow_dir=workflow_dir_path,
        default_branch=default_branch,
    )

    if post_release:
        modified = prep.post_release(update_workflows=update_workflows)
        action = "Post-release"
    else:
        modified = prep.prepare_release(update_workflows=update_workflows)
        action = "Release preparation"

    if modified:
        logging.info(f"{action} complete. Modified {len(modified)} file(s):")
        for path in modified:
            echo(f"  {path}")
    else:
        logging.warning(f"{action}: no files were modified.")


@gha_utils.command(short_help="Check if a version bump is allowed")
@option(
    "--part",
    type=Choice(["minor", "major"], case_sensitive=False),
    required=True,
    help="The version part to check for bump eligibility.",
)
def version_check(part: str) -> None:
    """Check if a version bump is allowed for the specified part.

    This command prevents double version increments within a development cycle.
    It compares the current version from pyproject.toml against the latest Git tag
    to determine if a bump has already been applied but not released.

    \b
    Examples:
        # Check if minor version bump is allowed
        gha-utils version-check --part minor

        # Check if major version bump is allowed
        gha-utils version-check --part major

    \b
    Output:
        - Prints "true" if the bump is allowed
        - Prints "false" if a bump of this type was already applied

    \b
    Use in GitHub Actions:
        allowed=$( gha-utils version-check --part minor )
        if [ "$allowed" = "true" ]; then
            bump-my-version bump minor
        fi
    """
    allowed = is_version_bump_allowed(part)  # type: ignore[arg-type]
    echo("true" if allowed else "false")


@gha_utils.group(short_help="Manage bundled configuration and templates")
def config():
    """Manage bundled configuration files and templates.

    This command group provides unified access to all bundled files:
    pyproject.toml templates, label definitions, and workflow templates.

    \b
    Subcommands:
        export - Export any bundled file (with smart default output paths)
        init   - Merge config into pyproject.toml

    \b
    Exportable files (gha-utils config export <filename>):
        ruff.toml, bumpversion.toml, ...      - pyproject.toml templates
        labels.toml                           - Label definitions
        labeller-file-based.yaml              - File-based labeller rules
        labeller-content-based.yaml           - Content-based labeller rules
        autofix.yaml, release.yaml, ...       - Workflow templates

    \b
    Examples:
        # Export with default output path
        gha-utils config export labels.toml
        gha-utils config export labeller-file-based.yaml
        gha-utils config export release.yaml

        # Export to custom path
        gha-utils config export labels.toml ./custom/labels.toml

        # Initialize config in pyproject.toml
        gha-utils config init ruff pyproject.toml

        # List all exportable files
        gha-utils config export --list
    """


@config.command(short_help="Export any bundled file")
@option(
    "--list",
    "list_only",
    is_flag=True,
    default=False,
    help="List all available exportable files with their default output paths.",
)
@argument(
    "filename",
    required=False,
    type=Choice(list(EXPORTABLE_FILES.keys()), case_sensitive=False),
)
@argument(
    "output_path",
    required=False,
    type=file_path(writable=True, resolve_path=True, allow_dash=True),
)
def export(list_only, filename, output_path):
    """Export any bundled file.

    Dumps the bundled file to a file or stdout. Each file has a default output
    path (shown with --list). Specify a custom path to override.

    \b
    Examples:
        # List all available files with default paths
        gha-utils config export --list

    \b
        # Export to default location
        gha-utils config export labels.toml
        gha-utils config export labeller-file-based.yaml
        gha-utils config export release.yaml

    \b
        # Export to custom location
        gha-utils config export labels.toml ./custom/labels.toml

    \b
        # Export to stdout (for pyproject.toml templates)
        gha-utils config export ruff.toml
    """
    if list_only:
        echo("Available files (with default output paths):")
        for file_id, default_path in EXPORTABLE_FILES.items():
            path_info = default_path if default_path else "(stdout)"
            echo(f"  {file_id} â†’ {path_info}")
        return

    if not filename:
        logging.error("Must specify a filename or use --list.")
        raise SystemExit(1)

    content = export_content(filename)

    # Use provided path, or fall back to default, or stdout.
    if output_path is None:
        default_path = get_default_output_path(filename)
        if default_path:
            output_path = Path(default_path)
        else:
            output_path = Path("-")

    if is_stdout(output_path):
        logging.info(f"Print to {sys.stdout.name}")
    else:
        logging.info(f"Write to {output_path}")

    echo(content.rstrip(), file=prep_path(output_path))


@config.command(short_help="Initialize config in pyproject.toml")
@argument(
    "config_type",
    type=Choice(list(INIT_CONFIGS.keys()), case_sensitive=False),
)
@option(
    "--source",
    type=file_path(exists=True, readable=True, resolve_path=True),
    default="pyproject.toml",
    help="Path to the pyproject.toml file to update.",
)
@argument(
    "output_path",
    type=file_path(writable=True, resolve_path=True, allow_dash=True),
    default="-",
)
@pass_context
def init(ctx, config_type, source, output_path):
    """Initialize a configuration by merging it into pyproject.toml.

    Reads pyproject.toml, checks if the [tool.X] section already exists,
    and if not, inserts the bundled template at the appropriate location.

    Only configs with [tool.X] sections support init: ruff, bumpversion.

    By default, outputs the merged result to stdout for preview. To update
    the file in-place, specify pyproject.toml as the output path.

    \b
    Examples:
        # Preview merged configuration (dry-run)
        gha-utils config init ruff

    \b
        # Update pyproject.toml in-place
        gha-utils config init ruff pyproject.toml

    \b
        # Initialize bumpversion config
        gha-utils config init bumpversion pyproject.toml
    """
    merged = init_config(config_type, source)

    if merged is None:
        cfg = INIT_CONFIGS[config_type]
        logging.warning(
            f"No changes needed. [{cfg.tool_section}] already exists."
        )
        ctx.exit()

    if is_stdout(output_path):
        logging.info(f"Print merged result to {sys.stdout.name}")
    else:
        logging.info(f"Write merged result to {output_path}")

    echo(merged.rstrip(), file=prep_path(output_path))


@gha_utils.command(short_help="Update Git's .mailmap file with missing contributors")
@option(
    "--source",
    type=file_path(readable=True, resolve_path=True),
    default=".mailmap",
    help="Mailmap source file to use as reference for contributors identities that "
    "are already grouped.",
)
@option(
    "--create-if-missing/--skip-if-missing",
    is_flag=True,
    default=True,
    help="If not found, either create the missing destination mailmap file, or skip "
    "the update process entirely. This option is ignored if the destination is to print "
    f"the result to {sys.stdout.name}.",
)
@argument(
    "destination_mailmap",
    type=file_path(writable=True, resolve_path=True, allow_dash=True),
    default="-",
)
@pass_context
def mailmap_sync(ctx, source, create_if_missing, destination_mailmap):
    """Update a ``.mailmap`` file with all missing contributors found in Git commit
    history.

    By default the ``.mailmap`` at the root of the repository is read and its content
    is reused as reference, so identities already aliased in there are preserved and
    used as initial mapping. Only missing contributors not found in this initial mapping
    are added.

    The resulting updated mapping is printed to the console output. So a bare call to
    `gha-utils mailmap-sync` is the same as a call to
    `gha-utils mailmap-sync --source .mailmap -`.

    To have the updated mapping written to a file, specify the output file like so:
    `gha-utils mailmap-sync .mailmap`.

    The updated results are sorted. But no attempts are made at regrouping new
    contributors. SO you have to edit entries by hand to regroup them
    """
    mailmap = Mailmap()

    if source.exists():
        logging.info(f"Read initial mapping from {source}")
        content = remove_header(source.read_text(encoding="UTF-8"))
        mailmap.parse(content)
    else:
        logging.debug(f"Mailmap source file {source} does not exists.")

    mailmap.update_from_git()
    new_content = mailmap.render()

    if is_stdout(destination_mailmap):
        logging.info(f"Print updated results to {sys.stdout.name}.")
        logging.debug(
            "Ignore the "
            + ("--create-if-missing" if create_if_missing else "--skip-if-missing")
            + " option."
        )
    else:
        logging.info(f"Save updated results to {destination_mailmap}")
        if not create_if_missing and not destination_mailmap.exists():
            logging.warning(
                f"{destination_mailmap} does not exists, stop the sync process."
            )
            ctx.exit()
        if content == new_content:
            logging.warning("Nothing to update, stop the sync process.")
            ctx.exit()

    echo(generate_header(ctx) + new_content, file=prep_path(destination_mailmap))


@gha_utils.command(short_help="Run a test plan from a file against a binary")
@option(
    "--command",
    "--binary",
    required=True,
    metavar="COMMAND",
    help="Path to the binary file to test, or a command line to be executed.",
)
@option(
    "-F",
    "--plan-file",
    type=file_path(exists=True, readable=True, resolve_path=True),
    multiple=True,
    metavar="FILE_PATH",
    help="Path to a test plan file in YAML. This option can be repeated to run "
    "multiple test plans in sequence. If not provided, a default test plan will be "
    "executed.",
)
@option(
    "-E",
    "--plan-envvar",
    multiple=True,
    metavar="ENVVAR_NAME",
    help="Name of an environment variable containing a test plan in YAML. This "
    "option can be repeated to collect multiple test plans.",
)
@option(
    "-t",
    "--select-test",
    type=IntRange(min=1),
    multiple=True,
    metavar="INTEGER",
    help="Only run the tests matching the provided test case numbers. This option can "
    "be repeated to run multiple test cases. If not provided, all test cases will be "
    "run.",
)
@option(
    "-s",
    "--skip-platform",
    type=Choice(sorted(ALL_IDS), case_sensitive=False),
    multiple=True,
    help="Skip tests for the specified platforms. This option can be repeated to "
    "skip multiple platforms.",
)
@option(
    "-x",
    "--exit-on-error",
    is_flag=True,
    default=False,
    help="Exit instantly on first failed test.",
)
@option(
    "-T",
    "--timeout",
    # Timeout passed to subprocess.run() is a float that is silently clamped to
    # 0.0 is negative values are provided, so we mimic this behavior here:
    # https://github.com/python/cpython/blob/5740b95076b57feb6293cda4f5504f706a7d622d/Lib/subprocess.py#L1596-L1597
    type=FloatRange(min=0, clamp=True),
    metavar="SECONDS",
    help="Set the default timeout for each CLI call, if not specified in the "
    "test plan.",
)
@option(
    "--show-trace-on-error/--hide-trace-on-error",
    default=True,
    help="Show execution trace of failed tests.",
)
@option(
    "--stats/--no-stats",
    is_flag=True,
    default=True,
    help="Print per-manager package statistics.",
)
def test_plan(
    command: str,
    plan_file: tuple[Path, ...] | None,
    plan_envvar: tuple[str, ...] | None,
    select_test: tuple[int, ...] | None,
    skip_platform: tuple[str, ...] | None,
    exit_on_error: bool,
    timeout: float | None,
    show_trace_on_error: bool,
    stats: bool,
) -> None:
    # Load test plan from workflow input, or use a default one.
    test_list = []
    if plan_file or plan_envvar:
        for file in unique(plan_file):
            logging.info(f"Get test plan from {file} file")
            tests = list(parse_test_plan(file.read_text(encoding="UTF-8")))
            logging.info(f"{len(tests)} test cases found.")
            test_list.extend(tests)
        for envvar_id in merge_envvar_ids(plan_envvar):
            logging.info(f"Get test plan from {envvar_id!r} environment variable")
            tests = list(parse_test_plan(os.getenv(envvar_id)))
            logging.info(f"{len(tests)} test cases found.")
            test_list.extend(tests)

    else:
        logging.warning(
            "No test plan provided through --plan-file/-F or --plan-envvar/-E options:"
            " use default test plan."
        )
        test_list = DEFAULT_TEST_PLAN
    logging.debug(f"Test plan: {test_list}")

    counter = Counter(total=len(test_list), skipped=0, failed=0)

    for index, test_case in enumerate(test_list):
        test_number = index + 1
        test_name = f"#{test_number}"
        logging.info(f"Run test {test_name}...")

        if select_test and test_number not in select_test:
            logging.warning(f"Test {test_name} skipped by user request.")
            counter["skipped"] += 1
            continue

        try:
            logging.debug(f"Test case parameters: {test_case}")
            test_case.run_cli_test(
                command,
                additional_skip_platforms=skip_platform,
                default_timeout=timeout,
            )
        except SkippedTest as ex:
            counter["skipped"] += 1
            logging.warning(f"Test {test_name} skipped: {ex}")
        except Exception as ex:
            counter["failed"] += 1
            logging.error(f"Test {test_name} failed: {ex}")
            if show_trace_on_error and test_case.execution_trace:
                echo(test_case.execution_trace)
            if exit_on_error:
                logging.debug("Don't continue testing, a failed test was found.")
                sys.exit(1)

    if stats:
        echo(
            "Test plan results - "
            + ", ".join((f"{k.title()}: {v}" for k, v in counter.items()))
        )

    if counter["failed"]:
        sys.exit(1)


@gha_utils.command(short_help="Label issues/PRs from GitHub sponsors")
@option(
    "--owner",
    help="GitHub username or organization to check sponsorship for. "
    "Defaults to $GITHUB_REPOSITORY_OWNER.",
)
@option(
    "--author",
    help="GitHub username of the issue/PR author to check. "
    "Defaults to author from $GITHUB_EVENT_PATH.",
)
@option(
    "--repo",
    help="Repository in 'owner/repo' format. Defaults to $GITHUB_REPOSITORY.",
)
@option(
    "--number",
    type=int,
    help="Issue or PR number. Defaults to number from $GITHUB_EVENT_PATH.",
)
@option(
    "--label",
    default="ðŸ’– sponsors",
    help="Label to add if author is a sponsor.",
)
@option(
    "--pr/--issue",
    "is_pr",
    default=None,
    help="Specify issue or pull request. Auto-detected from $GITHUB_EVENT_PATH.",
)
@pass_context
def sponsor_label(
    ctx: Context,
    owner: str | None,
    author: str | None,
    repo: str | None,
    number: int | None,
    label: str,
    is_pr: bool | None,
) -> None:
    """Add a label to issues or PRs from GitHub sponsors.

    Checks if the author of an issue or PR is a sponsor of the repository owner.
    If they are, adds the specified label.

    This command requires the ``gh`` CLI to be installed and authenticated.

    When run in GitHub Actions, all parameters are auto-detected from environment
    variables ($GITHUB_REPOSITORY_OWNER, $GITHUB_REPOSITORY) and the event payload
    ($GITHUB_EVENT_PATH). You can override any auto-detected value by passing it
    explicitly.

    \b
    Examples:
        # In GitHub Actions (all defaults auto-detected)
        gha-utils sponsor-label

    \b
        # Override specific values
        gha-utils sponsor-label --label "sponsor"

    \b
        # Manual invocation with all values
        gha-utils sponsor-label --owner kdeldycke --author some-user \\
            --repo kdeldycke/workflows --number 123 --issue
    """
    # Apply defaults from GitHub Actions environment.
    if owner is None:
        owner = get_default_owner()
    if author is None:
        author = get_default_author()
    if repo is None:
        repo = get_default_repo()
    if number is None:
        number = get_default_number()
    if is_pr is None:
        is_pr = is_pull_request()

    # Validate required parameters.
    missing = []
    if not owner:
        missing.append("--owner")
    if not author:
        missing.append("--author")
    if not repo:
        missing.append("--repo")
    if not number:
        missing.append("--number")

    if missing:
        logging.error(
            f"Missing required parameters: {', '.join(missing)}. "
            "These could not be auto-detected from the environment."
        )
        ctx.exit(1)

    # Type narrowing for mypy.
    assert owner and author and repo and number

    if is_sponsor(owner, author):
        if add_sponsor_label(repo, number, label, is_pr=is_pr):
            echo(f"Added {label!r} label to {'PR' if is_pr else 'issue'} #{number}")
        else:
            logging.error("Failed to add sponsor label")
            ctx.exit(1)
    else:
        echo(f"Author {author!r} is not a sponsor of {owner!r}")
