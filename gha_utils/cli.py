# Copyright Kevin Deldycke <kevin@deldycke.com> and contributors.
#
# This program is Free Software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.

from __future__ import annotations

import logging
import os
import re
import sys
from collections import Counter
from datetime import datetime
from pathlib import Path

from boltons.iterutils import unique
from click_extra import (
    Choice,
    Context,
    EnumChoice,
    FloatRange,
    IntRange,
    argument,
    dir_path,
    echo,
    file_path,
    group,
    option,
    pass_context,
)
from click_extra.envvar import merge_envvar_ids
from extra_platforms import ALL_IDS, is_github_ci

from . import __version__
from .changelog import Changelog
from .bundled_config import (
    EXPORTABLE_FILES,
    INIT_CONFIGS,
    export_content,
    get_default_output_path,
    init_config,
)
from .mailmap import Mailmap
from .metadata import NUITKA_BUILD_TARGETS, Dialect, Metadata, is_version_bump_allowed
from .release_prep import ReleasePrep
from .sponsor import (
    add_sponsor_label,
    get_default_author,
    get_default_number,
    get_default_owner,
    get_default_repo,
    is_pull_request,
    is_sponsor,
)
from .test_plan import DEFAULT_TEST_PLAN, SkippedTest, parse_test_plan
from .deps_graph import (
    generate_dependency_graph,
    get_available_extras,
    get_available_groups,
)
from .broken_links import manage_broken_links_issue
from .binary import (
    BINARY_ARCH_MAPPINGS,
    collect_and_rename_artifacts,
    format_github_output,
    verify_binary_arch,
)
from .git_ops import create_and_push_tag
from .renovate import (
    add_exclude_newer_to_file,
    calculate_target_date,
    has_tool_uv_section,
    parse_exclude_newer_date,
    run_renovate_prereq_checks,
    update_exclude_newer_in_file,
)
from .lint_repo import run_repo_lint

TYPE_CHECKING = False
if TYPE_CHECKING:
    from typing import IO


def is_stdout(filepath: Path) -> bool:
    """Check if a file path is set to stdout.

    Prevents the creation of a ``-`` file in the current directory.
    """
    return str(filepath) == "-"


def prep_path(filepath: Path) -> IO | None:
    """Prepare the output file parameter for Click's echo function."""
    if is_stdout(filepath):
        return None
    return filepath.open("w", encoding="UTF-8")


def generate_header(ctx: Context) -> str:
    """Generate metadata to be left as comments to the top of a file generated by
    this CLI.
    """
    header = (
        f"# Generated by {ctx.command_path} v{__version__}"
        " - https://github.com/kdeldycke/workflows\n"
        f"# Timestamp: {datetime.now().isoformat()}\n"
    )
    logging.debug(f"Generated header:\n{header}")
    return header


def remove_header(content: str) -> str:
    """Return the same content provided, but without the blank lines and header metadata generated by the function above."""
    logging.debug(f"Removing header from:\n{content}")
    lines = []
    still_in_header = True
    for line in content.splitlines():
        if still_in_header:
            # We are still in the header as long as we have blank lines or we have
            # comment lines matching the format produced by the method above.
            if not line.strip() or line.startswith((
                "# Generated by ",
                "# Timestamp: ",
            )):
                continue
            else:
                still_in_header = False
        # We are past the header, so keep all the lines: we have nothing left to remove.
        lines.append(line)

    headerless_content = "\n".join(lines)
    logging.debug(f"Result of header removal:\n{headerless_content}")
    return headerless_content


@group
def gha_utils():
    pass


@gha_utils.command(short_help="Output project metadata")
@option(
    "-u",
    "--unstable-targets",
    help="Build targets for which Nuitka is allowed to fail without comprimising the "
    " release workflow. This option accepts a mangled string with multiple targets "
    "separated by arbitrary separators. Recognized targets are: "
    f"{', '.join(NUITKA_BUILD_TARGETS)}.",
)
@option(
    "--format",
    type=EnumChoice(Dialect),
    default=Dialect.github,
    help="Rendering format of the metadata.",
)
@option(
    "--overwrite/--no-overwrite",
    "--force/--no-force",
    "--replace/--no-replace",
    default=True,
    help="Overwrite output file if it already exists.",
)
@option(
    "-o",
    "--output",
    type=file_path(writable=True, resolve_path=True, allow_dash=True),
    default="-",
    help="Output file path. Defaults to stdout.",
)
@pass_context
def metadata(ctx, unstable_targets, format, overwrite, output):
    """Dump project metadata to a file.

    By default the metadata produced are displayed directly to the console output.
    To have the results written in a file on disk, specify the output file like so:
    `gha-utils metadata --output dump.txt`.

    For GitHub you want to output to the standard environment file pointed to by the
    `$GITHUB_OUTPUT` variable. I.e.:

        $ gha-utils metadata --output "$GITHUB_OUTPUT"
    """
    if is_stdout(output):
        if overwrite:
            logging.warning("Ignore the --overwrite/--force/--replace option.")
        logging.info(f"Print metadata to {sys.stdout.name}")
    else:
        logging.info(f"Dump all metadata to {output}")

        if output.exists():
            msg = "Target file exist and will be overwritten."
            if overwrite:
                logging.warning(msg)
            else:
                logging.critical(msg)
                ctx.exit(2)

    # Extract targets from the raw string provided by the user.
    valid_targets = set()
    if unstable_targets:
        for target in re.split(r"[^a-z0-9\-]", unstable_targets.lower()):
            if target:
                if target not in NUITKA_BUILD_TARGETS:
                    logging.fatal(
                        f"Unrecognized {target!r} target. "
                        f"Must be one of {', '.join(NUITKA_BUILD_TARGETS)}."
                    )
                    sys.exit(1)
                valid_targets.add(target)
        logging.debug(
            f"Parsed {unstable_targets!r} string into {valid_targets} targets."
        )

    metadata = Metadata(valid_targets)

    # Output a warning in GitHub runners if metadata are not saved to $GITHUB_OUTPUT.
    if is_github_ci():
        env_file = os.getenv("GITHUB_OUTPUT")
        if env_file and Path(env_file) != output:
            logging.warning(
                "Output path is not the same as $GITHUB_OUTPUT environment variable,"
                " which is generally what we're looking to do in GitHub CI runners for"
                " other jobs to consume the produced metadata."
            )

    dialect = Dialect(format)
    content = metadata.dump(dialect=dialect)
    echo(content, file=prep_path(output))


@gha_utils.command(short_help="Maintain a Markdown-formatted changelog")
@option(
    "--source",
    type=file_path(exists=True, readable=True, resolve_path=True),
    default="changelog.md",
    help="Changelog source file in Markdown format.",
)
@argument(
    "changelog_path",
    type=file_path(writable=True, resolve_path=True, allow_dash=True),
    default="-",
)
@pass_context
def changelog(ctx, source, changelog_path):
    initial_content = None
    if source:
        logging.info(f"Read initial changelog from {source}")
        initial_content = source.read_text(encoding="UTF-8")

    changelog = Changelog(initial_content)
    content = changelog.update()
    if content == initial_content:
        logging.warning("Changelog already up to date. Do nothing.")
        ctx.exit()

    if is_stdout(changelog_path):
        logging.info(f"Print updated results to {sys.stdout.name}")
    else:
        logging.info(f"Save updated results to {changelog_path}")
    echo(content, file=prep_path(changelog_path))


@gha_utils.command(short_help="Prepare files for a release")
@option(
    "--changelog",
    "changelog_path",
    type=file_path(exists=True, readable=True, writable=True, resolve_path=True),
    default="changelog.md",
    help="Path to the changelog file.",
)
@option(
    "--citation",
    "citation_path",
    type=file_path(readable=True, writable=True, resolve_path=True),
    default="citation.cff",
    help="Path to the citation file.",
)
@option(
    "--workflow-dir",
    default=".github/workflows",
    help="Path to the GitHub workflows directory.",
)
@option(
    "--default-branch",
    default="main",
    help="Name of the default branch for workflow URL updates.",
)
@option(
    "--update-workflows/--no-update-workflows",
    default=False,
    help="Update workflow URLs to use versioned tag instead of default branch.",
)
@option(
    "--post-release",
    is_flag=True,
    default=False,
    help="Run post-release steps (retarget workflow URLs to default branch).",
)
@pass_context
def release_prep(
    ctx,
    changelog_path,
    citation_path,
    workflow_dir,
    default_branch,
    update_workflows,
    post_release,
):
    """Prepare files for a release or post-release version bump.

    This command consolidates all release preparation steps:

    \b
    - Set release date in changelog (replaces "(unreleased)" with today's date).
    - Set release date in citation.cff.
    - Update changelog comparison URL from "...main" to "...v{version}".
    - Remove the "[!IMPORTANT]" warning block from changelog.
    - Optionally update workflow URLs to use versioned tag.

    For post-release (after the release commit), use --post-release to retarget
    workflow URLs back to the default branch.

    Examples:

    \b
        # Prepare release (changelog + citation)
        gha-utils release-prep

    \b
        # Prepare release including workflow URL updates
        gha-utils release-prep --update-workflows

    \b
        # Post-release: retarget workflows to main branch
        gha-utils release-prep --post-release --update-workflows
    """
    workflow_dir_path = Path(workflow_dir).resolve() if workflow_dir else None
    prep = ReleasePrep(
        changelog_path=changelog_path,
        citation_path=citation_path if citation_path.exists() else None,
        workflow_dir=workflow_dir_path,
        default_branch=default_branch,
    )

    if post_release:
        modified = prep.post_release(update_workflows=update_workflows)
        action = "Post-release"
    else:
        modified = prep.prepare_release(update_workflows=update_workflows)
        action = "Release preparation"

    if modified:
        logging.info(f"{action} complete. Modified {len(modified)} file(s):")
        for path in modified:
            echo(f"  {path}")
    else:
        logging.warning(f"{action}: no files were modified.")


@gha_utils.command(short_help="Check if a version bump is allowed")
@option(
    "--part",
    type=Choice(["minor", "major"], case_sensitive=False),
    required=True,
    help="The version part to check for bump eligibility.",
)
def version_check(part: str) -> None:
    """Check if a version bump is allowed for the specified part.

    This command prevents double version increments within a development cycle.
    It compares the current version from pyproject.toml against the latest Git tag
    to determine if a bump has already been applied but not released.

    \b
    Examples:
        # Check if minor version bump is allowed
        gha-utils version-check --part minor

        # Check if major version bump is allowed
        gha-utils version-check --part major

    \b
    Output:
        - Prints "true" if the bump is allowed
        - Prints "false" if a bump of this type was already applied

    \b
    Use in GitHub Actions:
        allowed=$( gha-utils version-check --part minor )
        if [ "$allowed" = "true" ]; then
            bump-my-version bump minor
        fi
    """
    allowed = is_version_bump_allowed(part)  # type: ignore[arg-type]
    echo("true" if allowed else "false")


@gha_utils.group(short_help="Manage bundled configuration and templates")
def bundled():
    """Manage bundled configuration files and templates.

    This command group provides unified access to all bundled files:
    pyproject.toml templates, label definitions, and workflow templates.

    \b
    Subcommands:
        export - Export any bundled file (with smart default output paths)
        init   - Merge config into pyproject.toml

    \b
    Exportable files (gha-utils bundled export <filename>):
        ruff.toml, bumpversion.toml, ...      - pyproject.toml templates
        labels.toml                           - Label definitions
        labeller-file-based.yaml              - File-based labeller rules
        labeller-content-based.yaml           - Content-based labeller rules
        autofix.yaml, release.yaml, ...       - Workflow templates

    \b
    Examples:
        # Export with default output path
        gha-utils bundled export labels.toml
        gha-utils bundled export labeller-file-based.yaml
        gha-utils bundled export release.yaml

        # Export to custom path
        gha-utils bundled export labels.toml ./custom/labels.toml

        # Initialize config in pyproject.toml
        gha-utils bundled init ruff pyproject.toml

        # List all exportable files
        gha-utils bundled export --list
    """


@bundled.command(short_help="Export any bundled file")
@option(
    "--list",
    "list_only",
    is_flag=True,
    default=False,
    help="List all available exportable files with their default output paths.",
)
@argument(
    "filename",
    required=False,
    type=Choice(list(EXPORTABLE_FILES.keys()), case_sensitive=False),
)
@argument(
    "output_path",
    required=False,
    type=file_path(writable=True, resolve_path=True, allow_dash=True),
)
def export(list_only, filename, output_path):
    """Export any bundled file.

    Dumps the bundled file to a file or stdout. Each file has a default output
    path (shown with --list). Specify a custom path to override.

    \b
    Examples:
        # List all available files with default paths
        gha-utils bundled export --list

    \b
        # Export to default location
        gha-utils bundled export labels.toml
        gha-utils bundled export labeller-file-based.yaml
        gha-utils bundled export release.yaml

    \b
        # Export to custom location
        gha-utils bundled export labels.toml ./custom/labels.toml

    \b
        # Export to stdout (for pyproject.toml templates)
        gha-utils bundled export ruff.toml
    """
    if list_only:
        echo("Available files (with default output paths):")
        for file_id, default_path in EXPORTABLE_FILES.items():
            path_info = default_path if default_path else "(stdout)"
            echo(f"  {file_id} â†’ {path_info}")
        return

    if not filename:
        logging.error("Must specify a filename or use --list.")
        raise SystemExit(1)

    content = export_content(filename)

    # Use provided path, or fall back to default, or stdout.
    if output_path is None:
        default_path = get_default_output_path(filename)
        if default_path:
            output_path = Path(default_path)
        else:
            output_path = Path("-")

    if is_stdout(output_path):
        logging.info(f"Print to {sys.stdout.name}")
    else:
        logging.info(f"Write to {output_path}")

    echo(content.rstrip(), file=prep_path(output_path))


@bundled.command(short_help="Initialize config in pyproject.toml")
@argument(
    "config_type",
    type=Choice(list(INIT_CONFIGS.keys()), case_sensitive=False),
)
@option(
    "--source",
    type=file_path(exists=True, readable=True, resolve_path=True),
    default="pyproject.toml",
    help="Path to the pyproject.toml file to update.",
)
@argument(
    "output_path",
    type=file_path(writable=True, resolve_path=True, allow_dash=True),
    default="-",
)
@pass_context
def init(ctx, config_type, source, output_path):
    """Initialize a configuration by merging it into pyproject.toml.

    Reads pyproject.toml, checks if the [tool.X] section already exists,
    and if not, inserts the bundled template at the appropriate location.

    Only configs with [tool.X] sections support init: ruff, bumpversion.

    By default, outputs the merged result to stdout for preview. To update
    the file in-place, specify pyproject.toml as the output path.

    \b
    Examples:
        # Preview merged configuration (dry-run)
        gha-utils bundled init ruff

    \b
        # Update pyproject.toml in-place
        gha-utils bundled init ruff pyproject.toml

    \b
        # Initialize bumpversion config
        gha-utils bundled init bumpversion pyproject.toml
    """
    merged = init_config(config_type, source)

    if merged is None:
        cfg = INIT_CONFIGS[config_type]
        logging.warning(f"No changes needed. [{cfg.tool_section}] already exists.")
        ctx.exit()

    if is_stdout(output_path):
        logging.info(f"Print merged result to {sys.stdout.name}")
    else:
        logging.info(f"Write merged result to {output_path}")

    echo(merged.rstrip(), file=prep_path(output_path))


@gha_utils.command(short_help="Update Git's .mailmap file with missing contributors")
@option(
    "--source",
    type=file_path(readable=True, resolve_path=True),
    default=".mailmap",
    help="Mailmap source file to use as reference for contributors identities that "
    "are already grouped.",
)
@option(
    "--create-if-missing/--skip-if-missing",
    is_flag=True,
    default=True,
    help="If not found, either create the missing destination mailmap file, or skip "
    "the update process entirely. This option is ignored if the destination is to print "
    f"the result to {sys.stdout.name}.",
)
@argument(
    "destination_mailmap",
    type=file_path(writable=True, resolve_path=True, allow_dash=True),
    default="-",
)
@pass_context
def mailmap_sync(ctx, source, create_if_missing, destination_mailmap):
    """Update a ``.mailmap`` file with all missing contributors found in Git commit
    history.

    By default the ``.mailmap`` at the root of the repository is read and its content
    is reused as reference, so identities already aliased in there are preserved and
    used as initial mapping. Only missing contributors not found in this initial mapping
    are added.

    The resulting updated mapping is printed to the console output. So a bare call to
    `gha-utils mailmap-sync` is the same as a call to
    `gha-utils mailmap-sync --source .mailmap -`.

    To have the updated mapping written to a file, specify the output file like so:
    `gha-utils mailmap-sync .mailmap`.

    The updated results are sorted. But no attempts are made at regrouping new
    contributors. SO you have to edit entries by hand to regroup them
    """
    mailmap = Mailmap()

    if source.exists():
        logging.info(f"Read initial mapping from {source}")
        content = remove_header(source.read_text(encoding="UTF-8"))
        mailmap.parse(content)
    else:
        logging.debug(f"Mailmap source file {source} does not exists.")

    mailmap.update_from_git()
    new_content = mailmap.render()

    if is_stdout(destination_mailmap):
        logging.info(f"Print updated results to {sys.stdout.name}.")
        logging.debug(
            "Ignore the "
            + ("--create-if-missing" if create_if_missing else "--skip-if-missing")
            + " option."
        )
    else:
        logging.info(f"Save updated results to {destination_mailmap}")
        if not create_if_missing and not destination_mailmap.exists():
            logging.warning(
                f"{destination_mailmap} does not exists, stop the sync process."
            )
            ctx.exit()
        if content == new_content:
            logging.warning("Nothing to update, stop the sync process.")
            ctx.exit()

    echo(generate_header(ctx) + new_content, file=prep_path(destination_mailmap))


@gha_utils.command(short_help="Run a test plan from a file against a binary")
@option(
    "--command",
    "--binary",
    required=True,
    metavar="COMMAND",
    help="Path to the binary file to test, or a command line to be executed.",
)
@option(
    "-F",
    "--plan-file",
    type=file_path(exists=True, readable=True, resolve_path=True),
    multiple=True,
    metavar="FILE_PATH",
    help="Path to a test plan file in YAML. This option can be repeated to run "
    "multiple test plans in sequence. If not provided, a default test plan will be "
    "executed.",
)
@option(
    "-E",
    "--plan-envvar",
    multiple=True,
    metavar="ENVVAR_NAME",
    help="Name of an environment variable containing a test plan in YAML. This "
    "option can be repeated to collect multiple test plans.",
)
@option(
    "-t",
    "--select-test",
    type=IntRange(min=1),
    multiple=True,
    metavar="INTEGER",
    help="Only run the tests matching the provided test case numbers. This option can "
    "be repeated to run multiple test cases. If not provided, all test cases will be "
    "run.",
)
@option(
    "-s",
    "--skip-platform",
    type=Choice(sorted(ALL_IDS), case_sensitive=False),
    multiple=True,
    help="Skip tests for the specified platforms. This option can be repeated to "
    "skip multiple platforms.",
)
@option(
    "-x",
    "--exit-on-error",
    is_flag=True,
    default=False,
    help="Exit instantly on first failed test.",
)
@option(
    "-T",
    "--timeout",
    # Timeout passed to subprocess.run() is a float that is silently clamped to
    # 0.0 is negative values are provided, so we mimic this behavior here:
    # https://github.com/python/cpython/blob/5740b95076b57feb6293cda4f5504f706a7d622d/Lib/subprocess.py#L1596-L1597
    type=FloatRange(min=0, clamp=True),
    metavar="SECONDS",
    help="Set the default timeout for each CLI call, if not specified in the "
    "test plan.",
)
@option(
    "--show-trace-on-error/--hide-trace-on-error",
    default=True,
    help="Show execution trace of failed tests.",
)
@option(
    "--stats/--no-stats",
    is_flag=True,
    default=True,
    help="Print per-manager package statistics.",
)
def test_plan(
    command: str,
    plan_file: tuple[Path, ...] | None,
    plan_envvar: tuple[str, ...] | None,
    select_test: tuple[int, ...] | None,
    skip_platform: tuple[str, ...] | None,
    exit_on_error: bool,
    timeout: float | None,
    show_trace_on_error: bool,
    stats: bool,
) -> None:
    # Load test plan from workflow input, or use a default one.
    test_list = []
    if plan_file or plan_envvar:
        for file in unique(plan_file):
            logging.info(f"Get test plan from {file} file")
            tests = list(parse_test_plan(file.read_text(encoding="UTF-8")))
            logging.info(f"{len(tests)} test cases found.")
            test_list.extend(tests)
        for envvar_id in merge_envvar_ids(plan_envvar):
            logging.info(f"Get test plan from {envvar_id!r} environment variable")
            tests = list(parse_test_plan(os.getenv(envvar_id)))
            logging.info(f"{len(tests)} test cases found.")
            test_list.extend(tests)

    else:
        logging.warning(
            "No test plan provided through --plan-file/-F or --plan-envvar/-E options:"
            " use default test plan."
        )
        test_list = DEFAULT_TEST_PLAN
    logging.debug(f"Test plan: {test_list}")

    counter = Counter(total=len(test_list), skipped=0, failed=0)

    for index, test_case in enumerate(test_list):
        test_number = index + 1
        test_name = f"#{test_number}"
        logging.info(f"Run test {test_name}...")

        if select_test and test_number not in select_test:
            logging.warning(f"Test {test_name} skipped by user request.")
            counter["skipped"] += 1
            continue

        try:
            logging.debug(f"Test case parameters: {test_case}")
            test_case.run_cli_test(
                command,
                additional_skip_platforms=skip_platform,
                default_timeout=timeout,
            )
        except SkippedTest as ex:
            counter["skipped"] += 1
            logging.warning(f"Test {test_name} skipped: {ex}")
        except Exception as ex:
            counter["failed"] += 1
            logging.error(f"Test {test_name} failed: {ex}")
            if show_trace_on_error and test_case.execution_trace:
                echo(test_case.execution_trace)
            if exit_on_error:
                logging.debug("Don't continue testing, a failed test was found.")
                sys.exit(1)

    if stats:
        echo(
            "Test plan results - "
            + ", ".join((f"{k.title()}: {v}" for k, v in counter.items()))
        )

    if counter["failed"]:
        sys.exit(1)


@gha_utils.command(short_help="Label issues/PRs from GitHub sponsors")
@option(
    "--owner",
    help="GitHub username or organization to check sponsorship for. "
    "Defaults to $GITHUB_REPOSITORY_OWNER.",
)
@option(
    "--author",
    help="GitHub username of the issue/PR author to check. "
    "Defaults to author from $GITHUB_EVENT_PATH.",
)
@option(
    "--repo",
    help="Repository in 'owner/repo' format. Defaults to $GITHUB_REPOSITORY.",
)
@option(
    "--number",
    type=int,
    help="Issue or PR number. Defaults to number from $GITHUB_EVENT_PATH.",
)
@option(
    "--label",
    default="ðŸ’– sponsors",
    help="Label to add if author is a sponsor.",
)
@option(
    "--pr/--issue",
    "is_pr",
    default=None,
    help="Specify issue or pull request. Auto-detected from $GITHUB_EVENT_PATH.",
)
@pass_context
def sponsor_label(
    ctx: Context,
    owner: str | None,
    author: str | None,
    repo: str | None,
    number: int | None,
    label: str,
    is_pr: bool | None,
) -> None:
    """Add a label to issues or PRs from GitHub sponsors.

    Checks if the author of an issue or PR is a sponsor of the repository owner.
    If they are, adds the specified label.

    This command requires the ``gh`` CLI to be installed and authenticated.

    When run in GitHub Actions, all parameters are auto-detected from environment
    variables ($GITHUB_REPOSITORY_OWNER, $GITHUB_REPOSITORY) and the event payload
    ($GITHUB_EVENT_PATH). You can override any auto-detected value by passing it
    explicitly.

    \b
    Examples:
        # In GitHub Actions (all defaults auto-detected)
        gha-utils sponsor-label

    \b
        # Override specific values
        gha-utils sponsor-label --label "sponsor"

    \b
        # Manual invocation with all values
        gha-utils sponsor-label --owner kdeldycke --author some-user \\
            --repo kdeldycke/workflows --number 123 --issue
    """
    # Apply defaults from GitHub Actions environment.
    if owner is None:
        owner = get_default_owner()
    if author is None:
        author = get_default_author()
    if repo is None:
        repo = get_default_repo()
    if number is None:
        number = get_default_number()
    if is_pr is None:
        is_pr = is_pull_request()

    # Validate required parameters.
    missing = []
    if not owner:
        missing.append("--owner")
    if not author:
        missing.append("--author")
    if not repo:
        missing.append("--repo")
    if not number:
        missing.append("--number")

    if missing:
        logging.error(
            f"Missing required parameters: {', '.join(missing)}. "
            "These could not be auto-detected from the environment."
        )
        ctx.exit(1)

    # Type narrowing for mypy.
    assert owner and author and repo and number

    if is_sponsor(owner, author):
        if add_sponsor_label(repo, number, label, is_pr=is_pr):
            echo(f"Added {label!r} label to {'PR' if is_pr else 'issue'} #{number}")
        else:
            logging.error("Failed to add sponsor label")
            ctx.exit(1)
    else:
        echo(f"Author {author!r} is not a sponsor of {owner!r}")


@gha_utils.command(short_help="Generate dependency graph from uv lockfile")
@option(
    "-p",
    "--package",
    help="Focus on a specific package's dependency tree.",
)
@option(
    "-g",
    "--group",
    "groups",
    multiple=True,
    help="Include dependencies from the specified group (e.g., test, typing). "
    "Can be repeated.",
)
@option(
    "--all-groups",
    is_flag=True,
    default=False,
    help="Include all dependency groups from pyproject.toml.",
)
@option(
    "-e",
    "--extra",
    "extras",
    multiple=True,
    help="Include dependencies from the specified extra (e.g., xml, json5). "
    "Can be repeated.",
)
@option(
    "--all-extras",
    is_flag=True,
    default=False,
    help="Include all optional extras from pyproject.toml.",
)
@option(
    "--frozen/--no-frozen",
    default=True,
    help="Use --frozen to skip lock file updates.",
)
@option(
    "-l",
    "--level",
    type=int,
    default=None,
    help="Maximum depth of the dependency graph. "
    "1 = primary deps only, 2 = primary + their deps, etc.",
)
@option(
    "-o",
    "--output",
    type=file_path(writable=True, resolve_path=True, allow_dash=True),
    default="-",
    help="Output file path. Defaults to stdout.",
)
def deps_graph(
    package: str | None,
    groups: tuple[str, ...],
    all_groups: bool,
    extras: tuple[str, ...],
    all_extras: bool,
    frozen: bool,
    level: int | None,
    output: Path,
) -> None:
    """Generate a Mermaid dependency graph from the project's uv lockfile.

    Parses the CycloneDX SBOM export from uv and renders it as a Mermaid
    flowchart for documentation. Version specifiers from uv.lock are shown
    as edge labels.

    \b
    Examples:
        # Generate Mermaid graph
        gha-utils deps-graph

    \b
        # Include test dependencies
        gha-utils deps-graph --group test

    \b
        # Include all groups and extras
        gha-utils deps-graph --all-groups --all-extras

    \b
        # Focus on a specific package
        gha-utils deps-graph --package click-extra

    \b
        # Limit graph depth to 2 levels
        gha-utils deps-graph --level 2

    \b
        # Save to file
        gha-utils deps-graph --output docs/dependency-graph.md
    """
    # Resolve --all-groups and --all-extras flags.
    resolved_groups: tuple[str, ...] | None = groups if groups else None
    if all_groups:
        resolved_groups = get_available_groups()
        logging.info(f"Discovered groups: {', '.join(resolved_groups)}")

    resolved_extras: tuple[str, ...] | None = extras if extras else None
    if all_extras:
        resolved_extras = get_available_extras()
        logging.info(f"Discovered extras: {', '.join(resolved_extras)}")

    graph = generate_dependency_graph(
        package=package,
        groups=resolved_groups,
        extras=resolved_extras,
        frozen=frozen,
        depth=level,
    )

    if is_stdout(output):
        logging.info(f"Print graph to {sys.stdout.name}")
    else:
        logging.info(f"Write graph to {output}")

    echo(graph, file=prep_path(output))


@gha_utils.command(short_help="Manage broken links issue lifecycle")
@option(
    "--lychee-exit-code",
    type=int,
    required=True,
    help="Exit code from lychee (0=no broken links, 2=broken links found).",
)
@option(
    "--body-file",
    type=file_path(exists=True, readable=True, resolve_path=True),
    required=True,
    help="Path to the issue body file (lychee output).",
)
@option(
    "--repo-name",
    required=True,
    help="Repository name (for label selection).",
)
def broken_links(lychee_exit_code: int, body_file: Path, repo_name: str) -> None:
    """Manage the broken links issue lifecycle.

    This command consolidates the entire broken links workflow into a single call:

    \b
    1. Validates the lychee exit code (error if not 0 or 2).
    2. Lists open issues by github-actions[bot].
    3. Triages matching "Broken links" issues (keep newest, close duplicates).
    4. Closes duplicate/obsolete issues.
    5. Creates or updates the main issue.

    This command requires the ``gh`` CLI to be installed and authenticated.

    \b
    Examples:
        # Run after lychee check (broken links found)
        gha-utils broken-links \\
            --lychee-exit-code 2 \\
            --body-file ./lychee/out.md \\
            --repo-name "my-repo"

    \b
        # Run after lychee check (no broken links)
        gha-utils broken-links \\
            --lychee-exit-code 0 \\
            --body-file ./lychee/out.md \\
            --repo-name "my-repo"
    """
    manage_broken_links_issue(lychee_exit_code, body_file, repo_name)


@gha_utils.command(short_help="Verify binary architecture using exiftool")
@option(
    "--target",
    type=Choice(sorted(BINARY_ARCH_MAPPINGS.keys()), case_sensitive=False),
    required=True,
    help="Target platform (e.g., linux-arm64, macos-x64, windows-x64).",
)
@option(
    "--binary",
    "binary_path",
    type=file_path(exists=True, readable=True, resolve_path=True),
    required=True,
    help="Path to the binary file to verify.",
)
def verify_binary(target: str, binary_path: Path) -> None:
    """Verify that a compiled binary matches the expected architecture.

    Uses exiftool to inspect the binary and validates that its architecture
    matches what is expected for the specified target platform.

    Requires exiftool to be installed and available in PATH.

    \b
    Examples:
        # Verify a Linux ARM64 binary
        gha-utils verify-binary --target linux-arm64 --binary ./mpm-linux-arm64.bin

    \b
        # Verify a Windows x64 binary
        gha-utils verify-binary --target windows-x64 --binary ./mpm-windows-x64.exe
    """
    verify_binary_arch(target, binary_path)
    echo(f"Binary architecture verified for {target}: {binary_path}")


@gha_utils.command(short_help="Collect and rename artifacts for release")
@option(
    "--download-folder",
    type=dir_path(exists=True, resolve_path=True),
    required=True,
    help="Folder containing downloaded artifacts.",
)
@option(
    "--short-sha",
    required=True,
    help="SHA suffix to strip from binary filenames.",
)
@option(
    "--nuitka-matrix",
    default=None,
    help="JSON string of the nuitka matrix (to identify binaries).",
)
@option(
    "-o",
    "--output",
    type=file_path(writable=True, resolve_path=True, allow_dash=True),
    default="-",
    help="Output file path for GITHUB_OUTPUT format. Defaults to stdout.",
)
def collect_artifacts(
    download_folder: Path,
    short_sha: str,
    nuitka_matrix: str | None,
    output: Path,
) -> None:
    """Collect artifacts and rename binaries for GitHub release.

    Processes all files in the download folder:
    - Binaries (identified via nuitka-matrix) are renamed to strip the SHA suffix.
    - Other artifacts are collected as-is.

    Outputs artifact paths in GITHUB_OUTPUT multiline format.

    \b
    Examples:
        # Collect artifacts and write to GITHUB_OUTPUT
        gha-utils collect-artifacts \\
            --download-folder ./release_artifact \\
            --short-sha 346ce66 \\
            --nuitka-matrix '{"include": [...]}' \\
            --output "$GITHUB_OUTPUT"

    \b
        # Preview artifact collection (output to stdout)
        gha-utils collect-artifacts \\
            --download-folder ./release_artifact \\
            --short-sha abc1234
    """
    artifacts = collect_and_rename_artifacts(
        download_folder=download_folder,
        short_sha=short_sha,
        nuitka_matrix_json=nuitka_matrix,
    )

    github_output = format_github_output(artifacts)

    if is_stdout(output):
        logging.info(f"Print to {sys.stdout.name}")
    else:
        logging.info(f"Write to {output}")

    echo(github_output, file=prep_path(output))


@gha_utils.command(short_help="Update exclude-newer date in pyproject.toml")
@option(
    "--pyproject",
    type=file_path(resolve_path=True),
    default="pyproject.toml",
    help="Path to pyproject.toml file.",
)
@option(
    "--days",
    type=IntRange(min=1),
    default=7,
    help="Number of days in the past for the target date.",
)
@option(
    "-o",
    "--output",
    type=file_path(writable=True, resolve_path=True, allow_dash=True),
    default=None,
    help="Output file for modified=true/false (e.g., $GITHUB_OUTPUT).",
)
def update_exclude_newer(
    pyproject: Path,
    days: int,
    output: Path | None,
) -> None:
    """Update or add exclude-newer date in pyproject.toml.

    If [tool.uv] exists but exclude-newer is missing, adds it with a default
    value. If exclude-newer exists and is stale, updates it.

    Uses a fixed date (not relative) to prevent uv.lock timestamp churn on
    every sync. This ensures Renovate's minimumReleaseAge and uv's
    exclude-newer stay synchronized.

    \b
    Examples:
        # Check and update pyproject.toml
        gha-utils update-exclude-newer

    \b
        # Output result for GitHub Actions
        gha-utils update-exclude-newer --output "$GITHUB_OUTPUT"

    \b
        # Use a different cooldown period
        gha-utils update-exclude-newer --days 14
    """
    if not pyproject.exists():
        logging.info(f"No {pyproject} found, skipping.")
        if output:
            echo("modified=false", file=prep_path(output))
        return

    target_date = calculate_target_date(days)
    current_date = parse_exclude_newer_date(pyproject)

    logging.info(f"Current exclude-newer date: {current_date}")
    logging.info(f"Target date ({days} days ago): {target_date}")

    modified = False

    if current_date is None:
        # No exclude-newer found. Add it if [tool.uv] section exists.
        if has_tool_uv_section(pyproject):
            logging.info("Adding missing exclude-newer to [tool.uv] section.")
            modified = add_exclude_newer_to_file(pyproject, target_date)
        else:
            logging.info("No [tool.uv] section found, skipping.")
    elif current_date < target_date:
        # exclude-newer exists but is stale.
        modified = update_exclude_newer_in_file(pyproject, target_date)

    if output:
        echo(f"modified={'true' if modified else 'false'}", file=prep_path(output))


@gha_utils.command(short_help="Check Renovate prerequisites")
@option(
    "--repo",
    default=None,
    help="Repository in 'owner/repo' format. Defaults to $GITHUB_REPOSITORY.",
)
@option(
    "--sha",
    default=None,
    help="Commit SHA for permission checks. Defaults to $GITHUB_SHA.",
)
@pass_context
def check_renovate_prereqs(ctx: Context, repo: str | None, sha: str | None) -> None:
    """Check prerequisites for running Renovate.

    Validates that:
    - No Dependabot version updates config exists (.github/dependabot.yaml).
    - Dependabot security updates are disabled.
    - Token has commit statuses permission (non-fatal).

    \b
    Examples:
        # In GitHub Actions (all defaults auto-detected)
        gha-utils check-renovate-prereqs

    \b
        # Manual invocation
        gha-utils check-renovate-prereqs --repo owner/repo --sha abc123
    """
    # Apply defaults from environment.
    if repo is None:
        repo = os.getenv("GITHUB_REPOSITORY", "")
    if sha is None:
        sha = os.getenv("GITHUB_SHA", "")

    if not repo:
        logging.error("No repository specified. Set --repo or $GITHUB_REPOSITORY.")
        ctx.exit(1)
    if not sha:
        logging.error("No SHA specified. Set --sha or $GITHUB_SHA.")
        ctx.exit(1)

    exit_code = run_renovate_prereq_checks(repo, sha)
    ctx.exit(exit_code)


@gha_utils.command(short_help="Run repository consistency checks")
@option(
    "--package-name",
    default=None,
    help="Python package name from pyproject.toml.",
)
@option(
    "--repo-name",
    default=None,
    help="Repository name. Defaults to $GITHUB_REPOSITORY name component.",
)
@option(
    "--is-sphinx/--no-sphinx",
    default=False,
    help="Whether the project uses Sphinx documentation.",
)
@option(
    "--project-description",
    default=None,
    help="Project description from pyproject.toml.",
)
@option(
    "--repo",
    default=None,
    help="Repository in 'owner/repo' format. Defaults to $GITHUB_REPOSITORY.",
)
@pass_context
def lint_repo(
    ctx: Context,
    package_name: str | None,
    repo_name: str | None,
    is_sphinx: bool,
    project_description: str | None,
    repo: str | None,
) -> None:
    """Run consistency checks on repository metadata.

    Checks:
    - Package name vs repository name (warning).
    - Website field set for Sphinx projects (warning).
    - Repository description matches project description (error).

    \b
    Examples:
        # In GitHub Actions with metadata from previous job
        gha-utils lint-repo \\
            --package-name my-package \\
            --repo-name my-package \\
            --is-sphinx \\
            --project-description "A cool package."

    \b
        # Just check description
        gha-utils lint-repo --project-description "A cool package."
    """
    # Apply defaults from environment.
    if repo is None:
        repo = os.getenv("GITHUB_REPOSITORY", "")
    if repo_name is None and repo:
        # Extract repo name from owner/repo format.
        repo_name = repo.split("/")[-1] if "/" in repo else repo

    exit_code = run_repo_lint(
        package_name=package_name,
        repo_name=repo_name,
        is_sphinx=is_sphinx,
        project_description=project_description,
        repo=repo if repo else None,
    )
    ctx.exit(exit_code)


@gha_utils.command(short_help="Create and push a Git tag")
@option(
    "--tag",
    required=True,
    help="Tag name to create (e.g., v1.2.3).",
)
@option(
    "--commit",
    default=None,
    help="Commit to tag. Defaults to HEAD.",
)
@option(
    "--push/--no-push",
    default=True,
    help="Push the tag to remote after creation.",
)
@option(
    "--skip-existing/--error-existing",
    default=True,
    help="Skip silently if tag exists, or fail with an error.",
)
@option(
    "-o",
    "--output",
    type=file_path(writable=True, resolve_path=True, allow_dash=True),
    default=None,
    help="Output file for created=true/false (e.g., $GITHUB_OUTPUT).",
)
@pass_context
def git_tag(
    ctx: Context,
    tag: str,
    commit: str | None,
    push: bool,
    skip_existing: bool,
    output: Path | None,
) -> None:
    """Create and optionally push a Git tag.

    This command is idempotent: if the tag already exists and --skip-existing
    is used, it exits successfully without making changes. This allows safe
    re-runs of workflows interrupted after tag creation.

    \b
    Examples:
        # Create and push a tag
        gha-utils git-tag --tag v1.2.3

    \b
        # Tag a specific commit
        gha-utils git-tag --tag v1.2.3 --commit abc123def

    \b
        # Create tag without pushing
        gha-utils git-tag --tag v1.2.3 --no-push

    \b
        # Fail if tag exists
        gha-utils git-tag --tag v1.2.3 --error-existing

    \b
        # Output result for GitHub Actions
        gha-utils git-tag --tag v1.2.3 --output "$GITHUB_OUTPUT"
    """
    try:
        created = create_and_push_tag(
            tag=tag,
            commit=commit,
            push=push,
            skip_existing=skip_existing,
        )
        if created:
            echo(f"Created{'and pushed ' if push else ' '}tag {tag!r}")
        else:
            echo(f"Tag {tag!r} already exists, skipped.")

        if output:
            echo(f"created={'true' if created else 'false'}", file=prep_path(output))

    except ValueError as e:
        logging.error(str(e))
        ctx.exit(1)
    except Exception as e:
        logging.error(f"Failed to create/push tag: {e}")
        ctx.exit(1)
